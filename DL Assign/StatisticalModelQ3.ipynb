{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StatisticalModelQ3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNdpdNBl8PxvLmORPhAzOBF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OqdYvA-i8lyv","colab_type":"code","outputId":"4343b37a-2b7f-4b40-a181-0f28508918b4","executionInfo":{"status":"ok","timestamp":1588972713686,"user_tz":-60,"elapsed":8328,"user":{"displayName":"Shivam Khandelwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxOhJda74t7Mwm5x-yltVPBzh086c5Z0hLMrl1cA=s64","userId":"09963645253136722399"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["!pip install 'nltk==3.4.5'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting nltk==3.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.12.0)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449905 sha256=d07bd7f11dcc51c75f7a2fccd4eb5c42ff0e4c7e474abb02c65c0e76713b9a04\n","  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n","Successfully built nltk\n","Installing collected packages: nltk\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l-lMUPKW85u7","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.util import ngrams\n","from nltk.lm import MLE\n","from nltk import word_tokenize"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGTBp8NBabuk","colab_type":"text"},"source":["Now, we are trying to access the dataset that has been copied to drive"]},{"cell_type":"code","metadata":{"id":"CWBv6Q9CF_mo","colab_type":"code","outputId":"bed79035-db97-4363-ff90-acf213f6a298","executionInfo":{"status":"ok","timestamp":1588972883344,"user_tz":-60,"elapsed":177960,"user":{"displayName":"Shivam Khandelwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxOhJda74t7Mwm5x-yltVPBzh086c5Z0hLMrl1cA=s64","userId":"09963645253136722399"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vTSE5l3mMtgM","colab_type":"code","colab":{}},"source":["path = \"drive/My Drive/Colab Dataset/IMDB-Dataset.csv\"\n","IMDBFull = pd.read_csv(path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJ4N0ebOJFGQ","colab_type":"code","colab":{}},"source":["review=IMDBFull['review']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8L9kFwNapCM","colab_type":"text"},"source":["Below part of the code deals with tokenizing the long sequences to individual words and then padding the sentences."]},{"cell_type":"code","metadata":{"id":"niMbAvi1QzxQ","colab_type":"code","outputId":"69fc7793-e80b-47d3-d7da-a4f4e1f1e4bf","executionInfo":{"status":"ok","timestamp":1588972886871,"user_tz":-60,"elapsed":181465,"user":{"displayName":"Shivam Khandelwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxOhJda74t7Mwm5x-yltVPBzh086c5Z0hLMrl1cA=s64","userId":"09963645253136722399"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"pvPY_HLEQJTu","colab_type":"code","colab":{}},"source":["texts = []\n","for s in review:\n","    texts.append(word_tokenize(s))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTCcHlX9GHPY","colab_type":"code","colab":{}},"source":["from nltk.lm.preprocessing import padded_everygram_pipeline\n","train, vocab = padded_everygram_pipeline(3, texts)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2SbaCBRbxfp","colab_type":"text"},"source":["Till now N-grams for complete text have been generated, now we will look to fit the maximum likelihood model and use to generate 5 lines of variable length and random seed."]},{"cell_type":"code","metadata":{"id":"r87YpNEDEhH-","colab_type":"code","colab":{}},"source":["model = MLE(3) \n","model.fit(train, vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"McWTfwk4Fia4","colab_type":"code","outputId":"861e4125-5c39-4e11-b278-5f08c1f43e73","executionInfo":{"status":"ok","timestamp":1588973471526,"user_tz":-60,"elapsed":5672,"user":{"displayName":"Shivam Khandelwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxOhJda74t7Mwm5x-yltVPBzh086c5Z0hLMrl1cA=s64","userId":"09963645253136722399"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["word_list = StatModel.generate(10, random_seed=3)\n","print(' '.join(word for word in word_list))\n","\n","word_list = StatModel.generate(12, random_seed=7)\n","print(' '.join(word for word in word_list))\n","\n","word_list = StatModel.generate(8, random_seed=152)\n","print(' '.join(word for word in word_list))\n","\n","word_list = StatModel.generate(20, random_seed=225)\n","print(' '.join(word for word in word_list))\n","\n","word_list = StatModel.generate(10, random_seed=112)\n","print(' '.join(word for word in word_list))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mice and Men . He 's Not There '' .\n","alright , it 's low budget , however , is a bland\n","winds up being ticked ) in this movie\n","of attempting to trap people , unlikely the big , big Joe to give us a few bullets through his\n","ever seen . She understood that Danes ' character and\n"],"name":"stdout"}]}]}